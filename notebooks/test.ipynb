{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "620513a5",
   "metadata": {},
   "source": [
    "## Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3b937cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9184b2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Bonjour ! Comment puis-je vous aider aujourd'hui ?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 36, 'total_tokens': 48, 'completion_time': 0.01119315, 'prompt_time': 0.001795188, 'queue_time': 0.088909779, 'total_time': 0.012988338}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_7b3cfae3af', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--f78bc549-777a-40dd-9849-a2df9f936dc3-0', usage_metadata={'input_tokens': 36, 'output_tokens': 12, 'total_tokens': 48})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0,\n",
    "    max_tokens=64,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "llm.invoke(\"Bonjour\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e690343",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface.embeddings import HuggingFaceEndpointEmbeddings\n",
    "\n",
    "repo_id = \"google/embeddinggemma-300m\"\n",
    "\n",
    "embeddings = HuggingFaceEndpointEmbeddings(\n",
    "    repo_id=repo_id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f5546da",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9789359a",
   "metadata": {},
   "source": [
    "## Loading & processing docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c97db7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from langchain_core.documents import Document\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62cc8f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des abr√©viations\n",
    "ABBREVIATIONS = {\n",
    "    \"A\": \"Assistant\",\n",
    "    \"ALSH\": \"Arts Lettres et Sciences Humaines\",\n",
    "    \"CIDST\": \"Centre d'Information et de Documentation Scientifique et Technique\",\n",
    "    \"CNARP\": \"Centre National d'Application de Recherches Pharmaceutiques\",\n",
    "    \"CNRE\": \"Centre National de Recherche sur l'Environnement\",\n",
    "    \"CNRIT\": \"Centre National de Recherches Industrielle et Technologique\",\n",
    "    \"CNRO\": \"Centre National de Recherches Oc√©anographiques\",\n",
    "    \"CNTEMAD\": \"Centre National du T√©l√© Enseignement de Madagascar\",\n",
    "    \"ECD\": \"Employ√© de Courte Dur√©e\",\n",
    "    \"EFA\": \"Employ√© des Fonctionnaires Assimil√©s\",\n",
    "    \"ELD\": \"Employ√© de Longue Dur√©e\",\n",
    "    \"ESUP\": \"Enseignement Sup√©rieur\",\n",
    "    \"F\": \"F√©minin\",\n",
    "    \"FOFIFA\": \"Foibem-pirenena momba ny Fikarohana ampiharina amin'ny Fampandrosoana ny eny Ambanivohitra\",\n",
    "    \"IES\": \"Institut d'Enseignement Sup√©rieur\",\n",
    "    \"IMVAVET\": \"Institut Malgache des Vaccins V√©t√©rinaires\",\n",
    "    \"ISP\": \"Instituts Sup√©rieurs Priv√©s\",\n",
    "    \"IST\": \"Institut Sup√©rieur de Technologie\",\n",
    "    \"L1\": \"Licence Premi√®re ann√©e\",\n",
    "    \"L2\": \"Licence deuxi√®me ann√©e\",\n",
    "    \"L3\": \"Licence troisi√®me ann√©e\",\n",
    "    \"LMD\": \"Licence, Master, Doctorat\",\n",
    "    \"M\": \"Masculin\",\n",
    "    \"M1\": \"Master premi√®re ann√©e\",\n",
    "    \"M2\": \"Master deuxi√®me ann√©e\",\n",
    "    \"MC\": \"Ma√Ætre de Conf√©rence\",\n",
    "    \"P\": \"Professeur\",\n",
    "    \"PAT\": \"Personnel Administratif et Technique\",\n",
    "    \"PBZT\": \"Parc Botanique et Zoologique de Tsimbazaza\",\n",
    "    \"PE\": \"Personnel Enseignant\",\n",
    "    \"PIP\": \"Programme d'Investissement Public\",\n",
    "    \"PT\": \"Professeur Titulaire\",\n",
    "    \"PUB\": \"Public\",\n",
    "    \"S/T\": \"Sous total\",\n",
    "    \"SE\": \"Sciences de l'Education\",\n",
    "    \"SI\": \"Sciences de l'Ing√©nieur\",\n",
    "    \"SSANTE\": \"Sciences de la Sant√©\",\n",
    "    \"SSTE\": \"Sciences de la Soci√©t√©\",\n",
    "    \"STECH\": \"Sciences et Technologies\",\n",
    "    \"TA\": \"Taux d'Abandon\",\n",
    "    \"TP\": \"Taux de Promotion\",\n",
    "    \"TR\": \"Taux de Redoublement\"\n",
    "}\n",
    "\n",
    "\n",
    "with open('../data/abbreviations.json', 'w', encoding = \"utf-8\") as f:\n",
    "    json.dump(ABBREVIATIONS, f, ensure_ascii=False, indent = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f48e2f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du chemin du PDF\n",
    "data = \"../data/MESUPRES_en_chiffres_MAJ.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b64062f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class MESUPRESChunker:\n",
    "    \"\"\"\n",
    "    Chunker sp√©cialis√© pour MESUPRES\n",
    "    Cr√©e 3 types de chunks:\n",
    "    1. Titre + Tableau complet\n",
    "    2. Titre + Graphe + Analyse\n",
    "    3. Titres seuls (pour questions sur sujets)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pdf_path, abbreviations):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.abbreviations = abbreviations\n",
    "        self.chunks = []\n",
    "    \n",
    "    def extract_all_chunks(self):\n",
    "        \"\"\"Pipeline complet d'extraction\"\"\"\n",
    "        \n",
    "        with pdfplumber.open(self.pdf_path) as pdf:\n",
    "            for page_num, page in enumerate(pdf.pages, start=1):\n",
    "                text = page.extract_text()\n",
    "                if not text:\n",
    "                    continue\n",
    "                \n",
    "                # 1. Extraire chunks Tableau\n",
    "                table_chunks = self.extract_table_chunks(page, page_num, text)\n",
    "                self.chunks.extend(table_chunks)\n",
    "                \n",
    "                # 2. Extraire chunks Graphe\n",
    "                graph_chunks = self.extract_graph_chunks(page, page_num, text)\n",
    "                self.chunks.extend(graph_chunks)\n",
    "                \n",
    "                # 3. Extraire titres seuls\n",
    "                title_chunks = self.extract_title_chunks(text, page_num)\n",
    "                self.chunks.extend(title_chunks)\n",
    "        \n",
    "        return self.chunks\n",
    "    \n",
    "    def extract_table_chunks(self, page, page_num, text):\n",
    "        \"\"\"\n",
    "        Chunks Priorit√© 1: Titre + Tableau complet\n",
    "        Format: \"Tableau XX: TITRE\\n[donn√©es du tableau]\"\n",
    "        \"\"\"\n",
    "        \n",
    "        chunks = []\n",
    "        \n",
    "        # D√©tecter tous les tableaux de la page\n",
    "        tables = page.extract_tables()\n",
    "        \n",
    "        if not tables:\n",
    "            return chunks\n",
    "        \n",
    "        # Trouver les titres de tableaux dans le texte\n",
    "        tableau_pattern = r'(Tableau\\s+(\\d+)\\s*:\\s*([^\\n]+))'\n",
    "        tableau_matches = list(re.finditer(tableau_pattern, text, re.IGNORECASE))\n",
    "        \n",
    "        for table_idx, table in enumerate(tables):\n",
    "            if not table or len(table) < 2:\n",
    "                continue\n",
    "            \n",
    "            # Trouver le titre correspondant\n",
    "            title = \"\"\n",
    "            table_number = \"\"\n",
    "            \n",
    "            if table_idx < len(tableau_matches):\n",
    "                match = tableau_matches[table_idx]\n",
    "                title = match.group(1)  # \"Tableau 01: EVOLUTION...\"\n",
    "                table_number = match.group(2)  # \"01\"\n",
    "            \n",
    "            # Convertir le tableau en texte structur√©\n",
    "            table_text = self.table_to_structured_text(table)\n",
    "            \n",
    "            # Extraire aussi le contexte (phrase d'analyse apr√®s le tableau)\n",
    "            analysis = self.extract_post_table_analysis(text, title)\n",
    "            \n",
    "            # Assembler le chunk complet\n",
    "            chunk_content = f\"{title}\\n\\n{table_text}\"\n",
    "            if analysis:\n",
    "                chunk_content += f\"\\n\\nAnalyse: {analysis}\"\n",
    "            \n",
    "            # Extraire m√©tadonn√©es\n",
    "            keywords = self.extract_keywords(chunk_content)\n",
    "            years = self.extract_years(chunk_content)\n",
    "            regions = self.extract_regions(chunk_content)\n",
    "            \n",
    "            chunks.append(Document(\n",
    "                page_content=chunk_content,\n",
    "                metadata={\n",
    "                    'page': page_num,\n",
    "                    'type': 'table',\n",
    "                    'id': f\"Tableau {table_number}\" if table_number else f\"Tableau_p{page_num}_{table_idx}\",\n",
    "                    'keywords': keywords,\n",
    "                    'years': years,\n",
    "                    'regions': regions,\n",
    "                    'source': 'MESUPRES'\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def table_to_structured_text(self, table):\n",
    "        \"\"\"\n",
    "        Convertir tableau en texte structur√©\n",
    "        Garde la structure ligne par ligne pour extraction pr√©cise\n",
    "        \"\"\"\n",
    "        \n",
    "        if not table or len(table) < 2:\n",
    "            return \"\"\n",
    "        \n",
    "        lines = []\n",
    "        \n",
    "        # Headers (premi√®re ligne)\n",
    "        headers = [str(cell).strip() if cell else '' for cell in table[0]]\n",
    "        headers = [h if h else f\"Col{i}\" for i, h in enumerate(headers)]\n",
    "        \n",
    "        # Ligne de headers\n",
    "        lines.append(\" | \".join(headers))\n",
    "        lines.append(\"-\" * 80)  # S√©parateur\n",
    "        \n",
    "        # Lignes de donn√©es\n",
    "        for row in table[1:]:\n",
    "            row_clean = [str(cell).strip() if cell else '' for cell in row]\n",
    "            \n",
    "            # Ignorer lignes vides\n",
    "            if not any(row_clean):\n",
    "                continue\n",
    "            \n",
    "            # Format: \"Col1 | Col2 | Col3\"\n",
    "            lines.append(\" | \".join(row_clean))\n",
    "        \n",
    "        return \"\\n\".join(lines)\n",
    "    \n",
    "    def extract_post_table_analysis(self, text, table_title):\n",
    "        \"\"\"\n",
    "        Extraire la phrase d'analyse qui suit le tableau\n",
    "        Ex: \"Le nombre d'inscrits a enregistr√© une baisse de 12%...\"\n",
    "        \"\"\"\n",
    "        \n",
    "        if not table_title:\n",
    "            return \"\"\n",
    "        \n",
    "        # Chercher le texte apr√®s le titre du tableau\n",
    "        try:\n",
    "            title_pos = text.index(table_title)\n",
    "            after_title = text[title_pos + len(table_title):title_pos + 500]\n",
    "            \n",
    "            # Prendre la premi√®re phrase compl√®te\n",
    "            sentences = re.split(r'[.!?]\\s+', after_title)\n",
    "            if sentences and len(sentences[0]) > 20:\n",
    "                return sentences[0].strip()\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def extract_graph_chunks(self, page, page_num, text):\n",
    "        \"\"\"\n",
    "        Chunks Priorit√© 2: Titre Graphe + Analyse\n",
    "        Les graphes ne sont pas des images, mais des descriptions textuelles\n",
    "        \"\"\"\n",
    "        \n",
    "        chunks = []\n",
    "        \n",
    "        # Pattern pour d√©tecter les graphes\n",
    "        graphe_pattern = r'(Graphe?\\s+(\\d+)\\s*:\\s*([^\\n]+))'\n",
    "        matches = re.finditer(graphe_pattern, text, re.IGNORECASE)\n",
    "        \n",
    "        for match in matches:\n",
    "            title = match.group(1)  # \"Graphe 13: Taux de r√©partition...\"\n",
    "            graph_number = match.group(2)  # \"13\"\n",
    "            \n",
    "            # Extraire le contexte autour (200 chars avant et apr√®s)\n",
    "            start = max(0, match.start() - 200)\n",
    "            end = min(len(text), match.end() + 500)\n",
    "            context = text[start:end]\n",
    "            \n",
    "            # M√©tadonn√©es\n",
    "            keywords = self.extract_keywords(context)\n",
    "            years = self.extract_years(context)\n",
    "            regions = self.extract_regions(context)\n",
    "            \n",
    "            chunks.append(Document(\n",
    "                page_content=context,\n",
    "                metadata={\n",
    "                    'page': page_num,\n",
    "                    'type': 'graph',\n",
    "                    'id': f\"Graphe {graph_number}\",\n",
    "                    'keywords': keywords,\n",
    "                    'years': years,\n",
    "                    'regions': regions,\n",
    "                    'source': 'MESUPRES'\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def extract_title_chunks(self, text, page_num):\n",
    "        \"\"\"\n",
    "        Chunks Priorit√© 3: Titres seuls\n",
    "        Pour r√©pondre aux questions \"Quel est le sujet de...\"\n",
    "        \"\"\"\n",
    "        \n",
    "        chunks = []\n",
    "        \n",
    "        # Tableaux\n",
    "        for match in re.finditer(r'(Tableau\\s+\\d+\\s*:\\s*[^\\n]+)', text, re.IGNORECASE):\n",
    "            title = match.group(1).strip()\n",
    "            chunks.append(Document(\n",
    "                page_content=title,\n",
    "                metadata={\n",
    "                    'page': page_num,\n",
    "                    'type': 'title',\n",
    "                    'element_type': 'tableau',\n",
    "                    'source': 'MESUPRES'\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        # Graphes\n",
    "        for match in re.finditer(r'(Graphe?\\s+\\d+\\s*:\\s*[^\\n]+)', text, re.IGNORECASE):\n",
    "            title = match.group(1).strip()\n",
    "            chunks.append(Document(\n",
    "                page_content=title,\n",
    "                metadata={\n",
    "                    'page': page_num,\n",
    "                    'type': 'title',\n",
    "                    'element_type': 'graphe',\n",
    "                    'source': 'MESUPRES'\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def extract_keywords(self, text):\n",
    "        \"\"\"Extraire mots-cl√©s pertinents\"\"\"\n",
    "        keywords = []\n",
    "        \n",
    "        # Niveaux d'√©tudes\n",
    "        levels = ['L1', 'L2', 'L3', 'M1', 'M2', 'Doctorat', 'Licence', 'Master']\n",
    "        keywords.extend([l for l in levels if l in text])\n",
    "        \n",
    "        # Type d'√©tablissement\n",
    "        types = ['PUBLIC', 'PRIVE', 'ENSEMBLE']\n",
    "        keywords.extend([t for t in types if t in text])\n",
    "        \n",
    "        # Domaines\n",
    "        for abbr in ['SI', 'ALSH', 'SSTE', 'STECH', 'SSANTE', 'SE']:\n",
    "            if abbr in text:\n",
    "                keywords.append(abbr)\n",
    "        \n",
    "        return list(set(keywords))\n",
    "    \n",
    "    def extract_years(self, text):\n",
    "        \"\"\"Extraire ann√©es\"\"\"\n",
    "        years = re.findall(r'\\b(20\\d{2})\\b', text)\n",
    "        return sorted(list(set(map(int, years))))\n",
    "    \n",
    "    def extract_regions(self, text):\n",
    "        \"\"\"Extraire r√©gions\"\"\"\n",
    "        regions = [\n",
    "            'Antananarivo', 'Antsiranana', 'Diego', 'Fianarantsoa',\n",
    "            'Mahajanga', 'Toamasina', 'Toliara', 'Vatovavy Fitovinany',\n",
    "            'Alaotra Mangoro', 'Analanjirofo', 'Anosy', 'Vakinankaratra'\n",
    "        ]\n",
    "        found = [r for r in regions if r in text]\n",
    "        return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21776b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les abr√©viations\n",
    "with open('../data/abbreviations.json', 'r', encoding='utf-8') as f:\n",
    "    ABBREVIATIONS = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77a7baeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Extraction des chunks...\n",
      "‚úÖ 294 chunks cr√©√©s\n",
      "‚úÖ 294 chunks cr√©√©s\n"
     ]
    }
   ],
   "source": [
    "# Cr√©er le chunker et extraire les chunks\n",
    "chunker = MESUPRESChunker(\n",
    "    pdf_path=data,\n",
    "    abbreviations=ABBREVIATIONS\n",
    ")\n",
    "\n",
    "print(\"üîÑ Extraction des chunks...\")\n",
    "chunks = chunker.extract_all_chunks()\n",
    "print(f\"‚úÖ {len(chunks)} chunks cr√©√©s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0914c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä R√©partition:\n",
      "   - Tableaux: 35\n",
      "   - Graphes: 99\n",
      "   - Titres: 160\n"
     ]
    }
   ],
   "source": [
    "# Statistiques sur les chunks\n",
    "types = [c.metadata['type'] for c in chunks]\n",
    "print(f\"\\nüìä R√©partition:\")\n",
    "print(f\"   - Tableaux: {types.count('table')}\")\n",
    "print(f\"   - Graphes: {types.count('graph')}\")\n",
    "print(f\"   - Titres: {types.count('title')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
