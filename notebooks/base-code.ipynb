{"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Author : Tommy Lovaniaina RAMAROKOTO\n\nimport os\nfrom dotenv import load_dotenv, find_dotenv\n\nload_dotenv(find_dotenv())\n\ngemini_api_key = os.getenv(\"GEMINI_API_KEY\")\n\nif gemini_api_key:\n    os.environ[\"GEMINI_API_KEY\"] = gemini_api_key","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import List, Tuple, Dict\nfrom pypdf import PdfReader\nfrom tqdm import tqdm\nimport os\n\ndef load_pdf(file_path: str) -> Tuple[List[str], List[Dict]]:\n    \"\"\"\n    Reads text content from a PDF file, returns page texts and metadata.\n\n    Args:\n        file_path (str): Path to the PDF file.\n\n    Returns:\n        documents (List[str]): One string per page.\n        metadatas (List[Dict]): Metadata with filename + page number.\n    \"\"\"\n    reader = PdfReader(file_path)\n\n    documents = []\n    metadatas = []\n\n    for page_number, page in enumerate(\n        tqdm(reader.pages, desc=f\"Reading {os.path.basename(file_path)}\"), start=1\n    ):\n        text = page.extract_text()\n        if text and text.strip():\n            documents.append(text.strip())\n            metadatas.append({\n                \"filename\": os.path.basename(file_path),\n                \"page_number\": page_number               \n            })\n\n    return documents, metadatas\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loading data to vectorstore","metadata":{}},{"cell_type":"code","source":"from google import genai\nimport chromadb\nfrom chromadb.utils.embedding_functions import GoogleGenerativeAiEmbeddingFunction\nfrom tqdm import tqdm\nimport os\n\ndef load_data(documents, metadatas, collection_name):\n    \"\"\"\n    Loads all data (embeddings) into chromadb\n\n    Args: \n    documents (list[str]): list of all documents to load\n    collection_name (str): the name of the collection where the documents will be stored\n    \"\"\"\n    \n    client = chromadb.EphemeralClient()\n\n    google_api_key = None\n    if \"GEMINI_API_KEY\" not in os.environ:\n        gapikey = input(\"Please enter your Google API Key: \")\n        google_api_key = gapikey\n    else:\n        google_api_key = os.environ[\"GEMINI_API_KEY\"]\n\n    embedding_function = GoogleGenerativeAiEmbeddingFunction(\n        api_key=google_api_key\n    )\n\n    collection = client.get_or_create_collection(\n        name=collection_name, embedding_function=embedding_function\n    )\n\n    count = collection.count()\n    print(f\"Collection already contains {count} documents\")\n    ids = [str(i) for i in range(count, count + len(documents))]\n\n    # Load the documents in batches of 100\n    for i in tqdm(\n        range(0, len(documents), 100), desc=\"Adding documents\", unit_scale=100\n    ):\n        collection.add(\n            ids=ids[i : i + 100],\n            documents=documents[i : i + 100],\n            metadatas=metadatas[i : i + 100],\n        )\n    print(f\"Documents loaded successfully\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Getting DB by collection_name","metadata":{}},{"cell_type":"code","source":"import chromadb\n\ndef get_db(collection_name): \n    \"\"\"\n    Returns an instance of the db that match the collection_name\n    \"\"\"\n    google_api_key = None\n    if \"GEMINI_API_KEY\" not in os.environ:\n        gapikey = input(\"Please enter your Google API Key: \")\n        google_api_key = gapikey\n    else:\n        google_api_key = os.environ[\"GEMINI_API_KEY\"]\n\n    client = chromadb.EphemeralClient()\n\n    # create embedding function\n    embedding_function = GoogleGenerativeAiEmbeddingFunction(\n        api_key=google_api_key, task_type=\"RETRIEVAL_QUERY\"\n    )\n\n    db = client.get_collection(\n        name=collection_name, embedding_function=embedding_function\n    )\n    return db","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Retrieval","metadata":{}},{"cell_type":"code","source":"def get_relevant_passage(query, db, n_results):\n    results = db.query(\n        query_texts=[query], n_results=n_results, include=[\"documents\", \"metadatas\"]\n    )\n    return results['documents'][0], results['metadatas'][0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Generation","metadata":{}},{"cell_type":"code","source":"from typing import List, Union\n\ndef make_rag_prompt(query: str, relevant_passages: Union[str, List[str]]) -> str:\n    \"\"\"\n    Build a RAG prompt from query + relevant passages.\n\n    Args:\n        query (str): User query.\n        relevant_passages (str | List[str]): Either a single passage or a list of passages.\n\n    Returns:\n        str: Formatted prompt.\n    \"\"\"\n    # Normalize to list\n    if isinstance(relevant_passages, str):\n        relevant_passages = [relevant_passages]\n\n    # Clean and join passages\n    escaped_passages = []\n    for passage in relevant_passages:\n        cleaned = passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n        escaped_passages.append(cleaned)\n\n    combined_passages = \"\\n---\\n\".join(escaped_passages)\n\n    prompt = f\"\"\"You are a helpful and informative bot that answers questions using text from the reference passages below. \n                Your response must be direct, no need for preambule or not relevant phrases.  \n                If the passages are irrelevant to the answer, you may ignore them.\n                \n                QUESTION: '{query}'\n                PASSAGES:\n                {combined_passages}\n                \n                ANSWER:\n            \"\"\"\n    return prompt\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from typing import List\n\ndef get_gemini_response(query: str, context: List[str]) -> str:\n    \"\"\"\n    Queries the Gemini API to get a response to the question.\n\n    Args:\n    query (str): The original query.\n    context (List[str]): The context of the query, returned by embedding search.\n\n    Returns:\n    A response to the question.\n    \"\"\"\n    client = genai.Client()\n\n    response = client.models.generate_content(\n        model=\"gemini-2.5-flash-lite\",\n        contents=make_rag_prompt(query, context)\n    )\n\n    return response.text","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Bringing it all together","metadata":{}},{"cell_type":"code","source":"mesupres_data, mesupres_metadata = load_pdf(file_path=\"./data/MESUPRES_en_chiffres_MAJ.pdf\")\n\ncollection_name = 'rag'\nload_data(documents=mesupres_data, metadatas=mesupres_metadata, collection_name=collection_name)\n\ndb = get_db(collection_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport csv\n\ndef process_questions_from_csv(db, csv_path: str, output_csv: str = 'submission_file.csv'):\n    \"\"\"\n    Reads an XLSX file containing (id, question), generates answers,\n    and writes results to a CSV file.\n\n    Args:\n        db: The Chroma/Vector database client.\n        csv_path (str): Path to the CSV file containing questions.\n        output_csv (str): Path to the output CSV file.\n    \"\"\"\n    df = pd.read_csv(csv_path)\n\n    results = []\n\n    for _, row in df.iterrows():\n        qid = row[\"id\"]\n        question = row[\"question\"]\n        \n        print(f\"Answering question {qid}: {question}\")\n\n        relevant_texts, metadatas = get_relevant_passage(question, db, n_results=1)\n        answer = get_gemini_response(question, relevant_texts)\n        print(f\"Answer: {answer}\")\n\n        results.append({\n            \"id\": qid,\n            \"question\": question,\n            \"answer\": answer,\n            \"context\": relevant_texts,\n            \"ref_page\": metadatas[0][\"page_number\"]\n        })\n\n    with open(output_csv, mode=\"w\", newline=\"\", encoding=\"utf-8\") as f:\n        writer = csv.DictWriter(f, fieldnames=[\"id\", \"question\", \"answer\", \"context\", \"ref_page\"])\n        writer.writeheader()\n        for row in results:\n            writer.writerow({\n                \"id\": row[\"id\"],\n                \"question\": row[\"question\"],\n                \"answer\": row[\"answer\"],\n                \"context\": str(row[\"context\"]),\n                \"ref_page\": str(row[\"ref_page\"])\n            })\n\n    print(f\"âœ… Results written to {output_csv}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"process_questions_from_csv(db, './questions.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}